---
title: "Data Analytics Certification Notes"
author: "Shraman Gupta"
date: "`r format(Sys.time(), '%d %B, %Y, %X')`"
output:
    rmdformats::readthedown:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: true
    highlight: tango
    css: "style.css"
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction {.tabset}

These are the notes while preparing for AWS Certified Data Analytics Specialty 2022. In this journey, we will make important concept notes for following five sections for Data Analytics

* Collection
* Storage
* Processing
* Analysis
* Visualization
* Domain & Security

--------------------------------------------------------------------------

### Collection

* Kinesis (Data Streams,Producers,Consumers,Enhanced Fan Out,Scaling,Security,Data Firehose)
* SQS
* IoT
* Database Migration Service (DMS)
* Direct Connect
* Snow Family
* MSK (Managed Streaming, Connect, Serverless)

--------------------------------------------------------------------------

### Storage
* S3 (Storage,Lifecycle Rules,Versioning,Replication,Performance,Security,Event Notifications)
* DynamoDB (Basics,APIs,Indexes,PartiQL,DAX,Streams,TTL,Patterns,Security)
* ElastiCache (Fundamental)

--------------------------------------------------------------------------

### Processing
* Lambda
* Glue, Hive, ETL (Catalog, end-points, Costs)
* Glue Studio & DataBrew
* Glue Elastic Views
* Lake Formation
* Infrastructure (EMR, Hadoop, Serverless, Apache Spark)
* Spark integration with Kinesis & Redshift
* Applications on EMR (Hive,Pig,HBase,Presto,Zeppelin,Hue,Splunk,Flume)
* Data Pipeline
* Step Functions

--------------------------------------------------------------------------

### Analysis
* Kinesis Analytics
* OpenSearch
* Athena
* Redshift

--------------------------------------------------------------------------

### Visualization
* Quicksight (Pricing,Dashboards, ML Insights)

--------------------------------------------------------------------------

### Domain & Security
* S3 Encryption
* KMS (Basics,Key Rotation)
* Cloud HSM
* STS & Cross Account
* Identity Federation
* Policies
* CloudTrail
* VPC Endpoints

--------------------------------------------------------------------------

### Other Topics
* EC2 for Big Data
* AWS AppSync and Kendra
* AWS Data Exchange
* AWS AppFlow
* AWS Cleanup
* Sagemaker

--------------------------------------------------------------------------

## Collection {.tabset}

There are multiple ways for data collection in AWS. 

* **Real Time collection** - where we can perform action on our data
  * KDS, SQS, IoT - These services help you to react in real-time to events or data that is happening in your infrastructure

  
* **Near-real-time** - Reactive Actions
  * Firehose, DMS

  
* **Batch - Historical Analysis** - when we want to move large amount of data to perform some data analysis
  * Snowball, Data Pipeline

--------------------------------------------------------------------------

### Kinesis Data Streams

<u>**Overview**</u>

```{r echo=FALSE, out.width='100%'}
knitr::include_graphics('./images/kinesis_data_streams.png')
```

* Way to stream big data into your systems. It is made of multiple **shards** and this is something we need to provision ahead of time
* **Shard** - Data is split across all the shards and they define stream capacity in terms of ingestion and consumption rates
* **Producers** - Send data (**Produce Records**) into KDS and can be **manyfold**. Ex. applications, clients, SDK, KPL, Kinesis Agent. Data can be sent at the rate of **1 MB/s or 1000 msg/sec per shard**
* **Records** - is made of *partition key* and *data blob (upto 1 MB)*
  * Partition key - determines in which shard will the record go to
  * Data Blob - Value itself
* **Consumers** - Applications, Lambda functions, Firehose, Kinesis Data Analytics
  * When a consumer recieves a record, it receives a *patition key, sequence number (where the record was in shard), and data blob*
  * 2 MB/sec (shared) per shard all consumers
  * 2 MB/sec (enhanced) per shard per consumer

* **Properties:**
  * **Retention period** - 1 day to 1 year
  * Ability to reprocess data
  * Once data is inserted, it can't be deleted **(immutable)**
  * Data that shares same partition, goes to same shard

* **Capacity Modes:**
  * *Provisioned Mode* 
    * Choose # of shards and scale manually or through API
    * Input (1 MB/sec) & throughput (2 MB/sec both classic/enhanced)
    * Pay per shard provisioned per hour
  * *On-demand Mode*
    * No provision needed
    * Default capacity provisioned (4 MB/sec)
    * Automatic scaling (observed throughput peak during last 30 days)
    * Pay per stream per hour & data in/out per GB

* **Security:**
  * It is within region (where we have shards)
  * IAM polices for shards
  * Encryption
    * in-flight: HTTPS endpoints
    * at-rest: KMS
    * client side
  * VPC Endpoints available for Kinesis
  * Monitor API calls through CloudTrail

--------------------------------------------------------------------------

### Producers

How is the data ingested into Kinesis Streams. 

```{r echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics('./images/kinesis_producers.png')
```

* **SDK** - allows you to write code or use CLI to directly send data into Kinesis Streams
  * <u>*PutRecord(s) API*</u>
    * API to send one or more records
    * Uses batching and **increases** throughput (which means there will be less HTTP requests as we send many records as a part of one HTTP request)
    * Over the limits of throughput, we will get **ProvisionedThroughputExceeded**
    * SDK can be used in very different ways: Mobile SDK (Android, iOS)
    * **Used case** - in case of *low throughput*, need *higher latency* with simple API or just working directly from *Lambda*
    * **AWS Managed sources (uses SDK) for KDS** - CW logs, IoT, Kinesis Data Analytics
  * <u>*ProvisionedThroughputExceeded*</u>
    - It happens when we are sending more data than expected i.e. *exceeding MB/s or TPS for any shard*
    - Due to **hot shard** (partition key is corrupted and excess data into that partition). Need to distribute as much as possible
    - (+) Retries with backoff
    - (+) Increase shards (scaling)
    - (+) ensure partition key is good (distributed well)
* **Kinesis Producer Library (KPL)** - more advanced, write better code and has good features (for enhanced throughput)
  * Easy to use and highly configurable C++/Java library
  * Used for **building high performance, long running producers**
  * **Automated + configurable** retry mechanism (*Automatically deals with issue with API (SDK)*)
  * 2 Types of APIs:
    * Synchronous: Same as SDK
    * Asynchronous: Better performance for async process
  * **Submit** metrics to CW for monitoring
  * **Supports batching** - increased throughput + decrease cost (ON by default)
    * Collect Records + Write to multiple shards
    * Aggregate that increases latency i.e. *capability to store multiple records in one record* + *increase payload size and imporve throughput*
  * Compression (by user only) - *make records smaller*
  * To read **KPL Records**, we need KCL or special helper library (can't use CLI)
  * <u>**Batching**</u>
    * Let us say we are sending 2 KB of data to Kinesis Data streams using KPL
    * It won't be sent away on spot but **will wait for next records that might be coming** 
    * At one point, KPL can **aggregate** all records into one record and we can do it multiple times
    ```{r echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics('./images/kinesis_kpl.png')
```
    * And then to make it more efficient, it will Collect all aggregated records in PutRecords in one API call 
    * We can use **RecordMaxBufferedTime** that introduces some delay waiting for all records to go together in one API call **(default is 100ms)**
    * **WHEN NOT TO USE KPL** - Applciation that can't tolerate additional delay is not good use case here (need to use SDK directly here)

    ```{r echo=FALSE, out.width='550%', fig.align='center'}
knitr::include_graphics('./images/kinesis_notusekpl.png')
```
    
* **Kinesis Agent** - Linux program that runs on server to fetch log files and send reliably to Kinesis Streams
  * Java based agent and built upon KPL
  * Only Linux based system
  * Features:
    * Write from multiple directories and write to multiple streams
    * Routing feature based on directory / log file
    * Pre-process data before sending to streams
    * handles log file rotation, checkpointing and retry upon failures
    * Emits metrics to CW for monitoring
  
* **Third party libraries** - Spark, Flume, log4j, Kafka Connect, NiFi

### Consumers

### Enhanced Fan Out

### Scaling

### Security

### Kinesis Firehose

### SQS

### Kinesis Vs SQS


## Other Collections {.tabset}

This section includes IoT, Database Migration Service (DMS), Direct Connect, Snow Family, Managed Streaming for Kafka (MSK)

### IoT

### DMS

### Direct Connect

### Snow Family

### MSK

### Kinesis vs MSK